---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:1924
- loss:CosineSimilarityLoss
base_model: sentence-transformers/all-MiniLM-L6-v2
widget:
- source_sentence: i other skills courseskill name board year of passing grade intermediate
    grade drawing art examination committee 2011 b examination maharashtra state i
    academic programme detail national service scheme camp sponsored by government
    of india ministry of youth affairs and sports new delhi north maharashtra university
    jalgaon i strengths ability to learn from mistakes honesty ready to accept challenges
    and responsibilities quick learning adaptability confidence i declaration i 201
    your faithfully pia jetalal hiralal gorbanjaraeducation details may 2010 hsc nashik
    maharashtra state board june 2008 ssc nashik maharashtra state board asstprofessor
    asstprofessor skill details company details company kisan arts description total
    work experience in months sr name of organization college designation working
    period total experience no in months from to 1 kisan arts commerce science asstprofessor
    20072015 31052016 10 months sr college parola dist jalgaon political sci maharashtra
    company bahadarpur tai parola dist description
  sentences:
  - Hadoop
  - Operations Manager
  - Arts
- source_sentence: education details february 2006 to february 2006 tybcom commerce
    mumbai business analyst business analyst skill details company details company
    motilal oswal description business analyst handling it operation for institutional
    equities maintain daily mis in excel for cag research derivative sales team preparing
    auto dashboard for research sales trading team working on excel macro to create
    innovative report working on block related data working on bd fund from different
    geo working on investors corporate meeting to track corporate block fund interest
    in sector company fss description project description maintain and prepare cash
    indent cash report cash position and cash planning responsibilities maintain daily
    mis in excel provide complete information about mis atm maintain and prepare cash
    indent cash report cash position cash planning coordinate with bank cash department
    coordinate with custodians degree course year of passing grade institute university
    board percentage company angel broking description reporting assistant manager
    sr manger responsibilities handling team of 14 quality assurance team members
    maintain daily mis in excel of team productivity maintain and prepare repots adding
    comments on remark mailing client for modification of given number mailing reports
    to different branches coordinating with rm provide complete information about
    script to client
  sentences:
  - Java Developer
  - ETL Developer
  - Business Analyst
- source_sentence: good logical and analytical skills positive attitude towards solving
    problems and accepting challenges a team player leader a good organizer presonal
    details dob 20071995 marital status singleeducation details july 2015 to june
    2018 be electrical pune maharashtra sinhagad institute of technology january 2012
    diploma msbte kopargaon maharashtra in kbppolytechnic january 2010 ssc sgvidyalaya
    state board testing engineer electrical engineer skill details company details
    company intelux electronics pvt ltd description 1 power management system testing
    department
  sentences:
  - Testing
  - Sales
  - Python Developer
- source_sentence: core competencies maintain processes to ensure project management
    documentation reports and plans are relevant accurate and complete report automation
    dashboard preparation and sharing feedbacks basis on performance of project manager
    forecasting data regarding future risks project changes and updating the delivery
    team on timely basis good understanding of project management lifecycle proven
    excellence in risk management and control good understanding of software development
    lifecycle sdlc ability to synthesize qualitative and quantitative data quickly
    and draw meaningful insights knowledge of programmeproject management methodologies
    with full project reporting and governance ability to work with different crossfunctional
    stakeholders to establish and ensure a reliable and productive working relationship
    strong time management and organizational skills multitasking skills and ability
    to meet deadlines computer skills and certification advance knowledge in ms office
    2013 and macros skills strategic thinking and decision making ability sound analytical
    skills multitasking skills in fast paced environment leadership and inter personal
    skills strong information management ability particularly ms excel extraction
    formulae pivots and graphs education details january 2005 bachelor of business
    administration business administration pune maharashtra modern college hsc pune
    maharashtra sspms college ssc pune maharashtra saints high school pmo having an
    exp of 6 years experience in project management in it expertise in pmo team handling
    quality analyst proficient in data analyzing tools and techniques skill details
    documentation exprience 47 months governance exprience 19 months excel exprience
    6 months forecasting exprience 6 months ms excel exprience 6 monthscompany details
    company capita india pvt ltd description pune key result areas responsible for
    successful transition of knowledge system and operating capabilities for prudential
    multiclient pheonix royal london travelled onsite glasgow and being part with
    uk team to understand the transition pmo work process and execute successfully
    at offshore successfully transitioned work order management governance and reporting
    from uk lead a team of 6 members and follow up on the development of new ways
    of working documentation processes manage internal and external stakeholder engagement
    collaboration of teams and global pmos network helps achieve robust operations
    with all the resources and infrastructure to execute steady state operations company
    saviant technologies description for multiple projects established a pmo from
    scratch and provided seasoned leadership to the technical operations staff defined
    and implemented work priority management and resource management processes established
    a supportive environment that allowed employees to grow and provide imaginative
    solutions to complex client need track and monitor financial performance of the
    program report financials for actual to budgeted comparison for labor hours and
    dollars operating costs and capital costs secure funding approvals for changes
    in scope monitor program risks through an ongoing process of identifying assessing
    tracking developing and executing risk mitigation strategies reviewed project
    documentation and document lessons learned and provide recommendations to mitigate
    them in future projects risk identification mitigation strategy issue escalation
    client communication project timeline and resource management company infosys
    description pune key result areas responsible for resource management budgeting
    billing responsible for preparing and sharing different reports with delivery
    managers project managers quality team automation of reports for entire unit interpret
    data analyze results using statistical techniques and provide ongoing reports
    preparing case diagrams activity diagrams for various scenarios collate data study
    patterns and conduct brainstorming sessions to identify outliers review and approve
    project documentation assist in identification of risks in the project and setting
    up of mitigation plan of the risk by reviewing dashboards and reports customer
    feedback information and analysis reviews and validate the inputs from project
    mangers regarding dashboards and ppts supporting tl by training people on processdomain
    as a part of the growth plan sla compliance company capita india pvt ltd description
    pune key result areas audits reviews and validate the inputs from managers regarding
    dashboards and ppts auditing work done by onshore agents and simultaneously auditing
    work done for my old team and their reporting part as well assisting reporting
    manager in business transformation leadership skills with proven ability to influence
    and collaborate across all levels of the organization helping line managers to
    solve specific audit problems either on a onetoone basis or in groups reporting
    preparing weekly monthly quarterly yearly mis variance report performance report
    feedback analysis task activities report publish relevant business dashboards
    projects audit report
  sentences:
  - PMO
  - Java Developer
  - PMO
- source_sentence: technical skill set programming languages apache hadoop python
    shell scripting sql technologies hive pig sqoop flume oozie impala hdfs tools
    dataiku unravel cloudera putty hue cloudera manager eclipse resource manager initial
    learning program tata consultancy services june 2015 to august 2015 description
    this is a learning program conducted by tcs for the newly joined employees to
    accomplish them to learn the working standard of the organization during this
    period employee are groomed with various technical as well as ethical aspects
    education details be electronics communication indore madhya pradesh medicaps
    institute of technology management hadoop developer hadoophivesqoopflumepigmapreducepythonimpalasparkscalasqlunix
    skill details apache hadoop sqoop exprience 31 months hadoop exprience 31 months
    hadoop exprience 31 months hive exprience 31 months sqoop exprience 31 months
    python exprience less than 1 year months hdfs exprience less than 1 year months
    unix exprience less than 1 year months impala exprience less than 1 year months
    pig exprience less than 1 year months unravel exprience less than 1 year months
    mapreduce exprience less than 1 year months dataiku exprience less than 1 year
    monthscompany details company tata consultancy services description project description
    data warehouse division has multiple products for injecting storing analysing
    and presenting data the data lake program is started to provide multitalent secure
    data hub to store applications data on hadoop platform with strong data governance
    lineage auditing and monitoring capabilities the object of the project is to provide
    necessary engineering support to analytics and application teams so that they
    can focus on the business logic development in this project the major task is
    to set up the hadoop cluster and govern all the activities which are required
    for the smooth functioning of various hadoop ecosystems as the day and day data
    increasing so to provide stability to the ecosystem and smooth working of it developing
    and automating the various requirement specific utilities responsibility 1 developed
    proactive health check utility for data lake the utility proactively checks the
    smooth functioning of all hadoop components on the cluster and sends the result
    to email in html format the utility is being used for daily health checks as well
    as after upgrades 2 getting the data in different formats and processing the data
    in hadoop ecosystem after filtering the data using the appropriate techniques
    3 developed data pipeline utility to ingest data from rdbms database to hive external
    tables using sqoop commands the utility also offers the data quality check like
    row count validation 4 developed and automated various cluster health check usage
    capacity related reports using unix shell scripting 5 optimization of hive queries
    in order to increase the performance and minimize the hadoop resource utilizations
    6 creating flume agents to process the data to hadoop ecosystem side 7 performed
    benchmark testing on the hive queries and impala queries 8 involved in setting
    up the cluster and its components like edge node and ha implementation of the
    services hive server2 impala and hdfs 9 filtering the required data from available
    data using different technologies like pig regex serde etc 10 dataiku benchmark
    testing on top of impala and hive in compare to greenplum database 11 moving the
    data from greenplum database to hadoop side with help of sqoop pipeline process
    the data to hadoop side and storing the data into hive tables to do the performance
    testing 12 dealing with the hadoop ecosystem related issues in order to provide
    stability to wm hadoop ecosystem 13 rescheduling of job from autosys job hosting
    to tws job hosting for better performance declaration i hereby declare that the
    above mentioned information is authentic to the best of my knowledge company tata
    consultancy services description clients 1 barclays 2 union bank of california
    ubc 3 morgan stanley ms key projects handled project name absa reconciliations
    ubc and wmdatalake coe company tata consultancy services description project description
    migration of data from rdbms database to hive hadoop ecosystem hadoop platform
    ability with strong data governance lineage auditing and monitoring capabilities
    the objective of this project was to speed up the data processing so that the
    analysis and decision making become easy due to rdbms limitations to process waste
    amount of data at once and produce the results at the earliest client wanted to
    move the data to hadoop ecosystem so that they can overcome from those limitations
    and focus on business improvement only responsibility 1 optimising the sql queries
    for those data which were not required to move from rdbms to any other platform
    2 writing the hive queries and logic to move the data from rdbms to hadoop ecosystem
    3 writing the hive queries to analyse the required data as per the business requirements
    4 optimization of hive queries in order to increase the performance and minimize
    the hadoop resource utilizations 5 writing the sqoop commands and scripts to move
    the data from rdbms to hadoop side company tata consultancy services description
    project description create recs and migrating static setup of reconciliations
    from 81 version to 91 version of the environment intellimatch responsibility 1
    have worked on extracting business requirements analyzing and implementing them
    in developing recs 2 worked on migrating static setup of reconciliations from
    81 version to 91 version of the environment intellimatch 3 done the back end work
    where most of the things were related to writing the sql queries and provide the
    data for the new recs project name pso
  sentences:
  - Hadoop
  - Blockchain
  - Automation Testing
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer based on sentence-transformers/all-MiniLM-L6-v2

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) <!-- at revision c9745ed1d9f207416be6d2e6f8de32d1f16199bf -->
- **Maximum Sequence Length:** 256 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
  (2): Normalize()
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ðŸ¤— Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    'technical skill set programming languages apache hadoop python shell scripting sql technologies hive pig sqoop flume oozie impala hdfs tools dataiku unravel cloudera putty hue cloudera manager eclipse resource manager initial learning program tata consultancy services june 2015 to august 2015 description this is a learning program conducted by tcs for the newly joined employees to accomplish them to learn the working standard of the organization during this period employee are groomed with various technical as well as ethical aspects education details be electronics communication indore madhya pradesh medicaps institute of technology management hadoop developer hadoophivesqoopflumepigmapreducepythonimpalasparkscalasqlunix skill details apache hadoop sqoop exprience 31 months hadoop exprience 31 months hadoop exprience 31 months hive exprience 31 months sqoop exprience 31 months python exprience less than 1 year months hdfs exprience less than 1 year months unix exprience less than 1 year months impala exprience less than 1 year months pig exprience less than 1 year months unravel exprience less than 1 year months mapreduce exprience less than 1 year months dataiku exprience less than 1 year monthscompany details company tata consultancy services description project description data warehouse division has multiple products for injecting storing analysing and presenting data the data lake program is started to provide multitalent secure data hub to store applications data on hadoop platform with strong data governance lineage auditing and monitoring capabilities the object of the project is to provide necessary engineering support to analytics and application teams so that they can focus on the business logic development in this project the major task is to set up the hadoop cluster and govern all the activities which are required for the smooth functioning of various hadoop ecosystems as the day and day data increasing so to provide stability to the ecosystem and smooth working of it developing and automating the various requirement specific utilities responsibility 1 developed proactive health check utility for data lake the utility proactively checks the smooth functioning of all hadoop components on the cluster and sends the result to email in html format the utility is being used for daily health checks as well as after upgrades 2 getting the data in different formats and processing the data in hadoop ecosystem after filtering the data using the appropriate techniques 3 developed data pipeline utility to ingest data from rdbms database to hive external tables using sqoop commands the utility also offers the data quality check like row count validation 4 developed and automated various cluster health check usage capacity related reports using unix shell scripting 5 optimization of hive queries in order to increase the performance and minimize the hadoop resource utilizations 6 creating flume agents to process the data to hadoop ecosystem side 7 performed benchmark testing on the hive queries and impala queries 8 involved in setting up the cluster and its components like edge node and ha implementation of the services hive server2 impala and hdfs 9 filtering the required data from available data using different technologies like pig regex serde etc 10 dataiku benchmark testing on top of impala and hive in compare to greenplum database 11 moving the data from greenplum database to hadoop side with help of sqoop pipeline process the data to hadoop side and storing the data into hive tables to do the performance testing 12 dealing with the hadoop ecosystem related issues in order to provide stability to wm hadoop ecosystem 13 rescheduling of job from autosys job hosting to tws job hosting for better performance declaration i hereby declare that the above mentioned information is authentic to the best of my knowledge company tata consultancy services description clients 1 barclays 2 union bank of california ubc 3 morgan stanley ms key projects handled project name absa reconciliations ubc and wmdatalake coe company tata consultancy services description project description migration of data from rdbms database to hive hadoop ecosystem hadoop platform ability with strong data governance lineage auditing and monitoring capabilities the objective of this project was to speed up the data processing so that the analysis and decision making become easy due to rdbms limitations to process waste amount of data at once and produce the results at the earliest client wanted to move the data to hadoop ecosystem so that they can overcome from those limitations and focus on business improvement only responsibility 1 optimising the sql queries for those data which were not required to move from rdbms to any other platform 2 writing the hive queries and logic to move the data from rdbms to hadoop ecosystem 3 writing the hive queries to analyse the required data as per the business requirements 4 optimization of hive queries in order to increase the performance and minimize the hadoop resource utilizations 5 writing the sqoop commands and scripts to move the data from rdbms to hadoop side company tata consultancy services description project description create recs and migrating static setup of reconciliations from 81 version to 91 version of the environment intellimatch responsibility 1 have worked on extracting business requirements analyzing and implementing them in developing recs 2 worked on migrating static setup of reconciliations from 81 version to 91 version of the environment intellimatch 3 done the back end work where most of the things were related to writing the sql queries and provide the data for the new recs project name pso',
    'Hadoop',
    'Automation Testing',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 1,924 training samples
* Columns: <code>sentence_0</code>, <code>sentence_1</code>, and <code>label</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                           | sentence_1                                                                      | label                                                         |
  |:--------|:-------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------|:--------------------------------------------------------------|
  | type    | string                                                                               | string                                                                          | float                                                         |
  | details | <ul><li>min: 21 tokens</li><li>mean: 218.42 tokens</li><li>max: 256 tokens</li></ul> | <ul><li>min: 3 tokens</li><li>mean: 4.01 tokens</li><li>max: 5 tokens</li></ul> | <ul><li>min: 0.0</li><li>mean: 0.5</li><li>max: 1.0</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | sentence_1                   | label            |
  |:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------|:-----------------|
  | <code>skill sets multitasking collaborative optimistic thinking effective teamleaderteam trainer visualizing the work which is to be done good grip on communication various languages known value loyalty and is loyal towards my responsibility compatible working with ms officeeducation details january 2017 mba marketing sales amity university january 2015 bsc hotel management psg college of arts and sciences institutionuniversityboard sales manager sales and marketing management skill details ms office exprience 4 monthscompany details company cohesive technologies description i am responsible for managing companys business in mumbai areaalong with branch manageri follow up leads given to me by my corporate office i meet them personally give them the best solutions and product suitable for their business i even generate leads by myself by calling up the clients through cold callmy job is like an entrepreneur here which basically involves managing a small business</code>                                   | <code>Sales</code>           | <code>1.0</code> |
  | <code>education details january 2018 bachelors of engineering engineering mumbai maharashtra mgm college of engineering diploma mechanical pune maharashtra mit president of mechanical engineering students association president of mechanical engineering students association skill details company details company full throttle description conducted by iit bombay worked as a president of mechanical engineering students association mesa in mit pune company rc car race of stepcone description paper project contest and exhibition conducted by gmr instiute of technology</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                          | <code>ETL Developer</code>   | <code>0.0</code> |
  | <code>skills visa b1visa usa onsite visits to sweden us seattle education details january 2013 post graduate diploma information technology pune maharashtra symbiosis institute january 2007 bachelor of engineering electronics and telecommunications pune maharashtra pune university cloud operations architect devops cloud operations architect devops devops skill details cloud computing exprience 48 months shell scripting exprience 96 months python exprience 6 months automation exprience 72 months solution architect exprience less than 1 year months azure exprience less than 1 year months aws exprience less than 1 year monthscompany details company devops description type devops engineer platform aws cloud azure cloud services aws ec2 rds cloudformation template lambda dynamo db cloud watch autoscaling elastic bean stalk appdynamics here i manage tibco spotfire enterprise cloud product support being the only ops member in india i got a chance to recruit build entire team of 15 members i also wo...</code> | <code>DevOps Engineer</code> | <code>1.0</code> |
* Loss: [<code>CosineSimilarityLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#cosinesimilarityloss) with these parameters:
  ```json
  {
      "loss_fct": "torch.nn.modules.loss.MSELoss"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 3
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `tp_size`: 0
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Framework Versions
- Python: 3.11.9
- Sentence Transformers: 4.1.0
- Transformers: 4.51.3
- PyTorch: 2.7.0+cpu
- Accelerate: 1.6.0
- Datasets: 3.6.0
- Tokenizers: 0.21.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->